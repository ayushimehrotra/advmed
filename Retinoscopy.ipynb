{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e7c08fc-427e-4b56-9691-8bc8d7ed9702",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72e57562-bd28-48ca-9e3c-46c9e80cbaa1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mmkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
      "Downloading diabetic-retinopathy-resized.zip to /root\n",
      "100%|██████████████████████████████████████| 7.25G/7.25G [04:16<00:00, 30.2MB/s]\n",
      "100%|██████████████████████████████████████| 7.25G/7.25G [04:16<00:00, 30.3MB/s]\n"
     ]
    }
   ],
   "source": [
    "!pip install -q kaggle\n",
    "!mkdir ~/.kaggle\n",
    "!touch ~/.kaggle/kaggle.json\n",
    "\n",
    "api_token = {\"username\":\"ayushimehrotra123\",\"key\":\"5739f2d0b63b057c29700575e302ac01\"}\n",
    "\n",
    "import json\n",
    "\n",
    "with open('/root/.kaggle/kaggle.json', 'w') as file:\n",
    "    json.dump(api_token, file)\n",
    "\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "!kaggle datasets download -d tanlikesmath/diabetic-retinopathy-resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75892012-6589-4844-96fa-9e4df828a7eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import zipfile \n",
    "with zipfile.ZipFile('./diabetic-retinopathy-resized.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('./eye-cancer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a91b52-89b9-489f-9916-5a9ab873023d",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "531da182-1ecc-43b6-b3cb-5f552bdbd5a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "!pip install torch\n",
    "!pip install torchvision\n",
    "!pip install captum\n",
    "!pip install opencv-python\n",
    "!pip install seaborn\n",
    "!pip install matplotlib\n",
    "!pip install cleverhans\n",
    "!pip install cachetools\n",
    "!pip install pandas\n",
    "!apt-get update && apt-get install ffmpeg libsm6 libxext6  -y\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ba50c878-df8e-49c2-8840-0bcd098fd351",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "import seaborn as sns\n",
    "import random\n",
    "import cv2\n",
    "import seaborn as sea\n",
    "import copy\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim,nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.utils import make_grid\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "# from mlxtend.plotting import plot_confusion_matrix\n",
    "# from sklearn.metrics import confusion_matrix\n",
    " \n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc06a7fa-e927-462a-8bb3-64f88bcb6df8",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0c149a52-7cf0-45fc-b1ae-e1ccc7423559",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed = 23):\n",
    "    # tests\n",
    "    assert isinstance(seed, int), 'seed has to be an integer'\n",
    "    \n",
    "    # randomness\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "image_size = 256\n",
    "#IMAGE PREPROCESSING\n",
    "\n",
    "def prepare_image(path, \n",
    "                  sigmaX         = 10, \n",
    "                  do_random_crop = False):\n",
    "    \n",
    "    '''\n",
    "    Preprocess image\n",
    "    '''\n",
    "    \n",
    "    # import image\n",
    "    image = cv2.imread(path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # perform smart crops\n",
    "    image = crop_black(image, tol = 7)\n",
    "    if do_random_crop == True:\n",
    "        image = random_crop(image, size = (0.9, 1))\n",
    "    \n",
    "    # resize and color\n",
    "    image = cv2.resize(image, (int(image_size), int(image_size)))\n",
    "    image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), sigmaX), -4, 128)\n",
    "    \n",
    "    # circular crop\n",
    "    image = circle_crop(image, sigmaX = sigmaX)\n",
    "\n",
    "    # convert to tensor    \n",
    "    image = torch.tensor(image)\n",
    "    image = image.permute(2, 1, 0)\n",
    "    return image\n",
    "\n",
    "def crop_black(img, \n",
    "               tol = 7):\n",
    "    \n",
    "    '''\n",
    "    Perform automatic crop of black areas\n",
    "    '''\n",
    "    \n",
    "    if img.ndim == 2:\n",
    "        mask = img > tol\n",
    "        return img[np.ix_(mask.any(1),mask.any(0))]\n",
    "    \n",
    "    elif img.ndim == 3:\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        mask = gray_img > tol\n",
    "        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n",
    "        \n",
    "        if (check_shape == 0): \n",
    "            return img \n",
    "        else:\n",
    "            img1 = img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img2 = img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img3 = img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img  = np.stack([img1, img2, img3], axis = -1)\n",
    "            return img\n",
    "\n",
    "def circle_crop(img, \n",
    "                sigmaX = 10):   \n",
    "    \n",
    "    '''\n",
    "    Perform circular crop around image center\n",
    "    '''\n",
    "        \n",
    "    height, width, depth = img.shape\n",
    "    \n",
    "    largest_side = np.max((height, width))\n",
    "    img = cv2.resize(img, (largest_side, largest_side))\n",
    "\n",
    "    height, width, depth = img.shape\n",
    "    \n",
    "    x = int(width / 2)\n",
    "    y = int(height / 2)\n",
    "    r = np.amin((x,y))\n",
    "    \n",
    "    circle_img = np.zeros((height, width), np.uint8)\n",
    "    cv2.circle(circle_img, (x,y), int(r), 1, thickness = -1)\n",
    "    \n",
    "    img = cv2.bitwise_and(img, img, mask = circle_img)\n",
    "    return img \n",
    "\n",
    "def random_crop(img, \n",
    "                size = (0.9, 1)):\n",
    "    \n",
    "    '''\n",
    "    Random crop\n",
    "    '''\n",
    "\n",
    "    height, width, depth = img.shape\n",
    "    \n",
    "    cut = 1 - random.uniform(size[0], size[1])\n",
    "    \n",
    "    i = random.randint(0, int(cut * height))\n",
    "    j = random.randint(0, int(cut * width))\n",
    "    h = i + int((1 - cut) * height)\n",
    "    w = j + int((1 - cut) * width)\n",
    "\n",
    "    img = img[i:h, j:w, :]    \n",
    "    \n",
    "    return img\n",
    "\n",
    "class EyeData(Dataset):\n",
    "    \n",
    "    # initialize\n",
    "    def __init__(self, data, directory, transform = None, do_random_crop = True, itype = '.png'):\n",
    "        self.data      = data\n",
    "        self.directory = directory\n",
    "        self.transform = transform\n",
    "        self.do_random_crop = do_random_crop\n",
    "        self.itype = itype\n",
    "    # length\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    # get items    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.directory, self.data.loc[idx, 'id_code'] + self.itype)\n",
    "        image    = prepare_image(img_name, do_random_crop = self.do_random_crop)\n",
    "        image    = self.transform(image)\n",
    "        label    = torch.tensor(self.data.loc[idx, 'diagnosis'])\n",
    "        return {'image': image, 'label': label}\n",
    "    \n",
    "class Data(Dataset):\n",
    "    \n",
    "    # initialize\n",
    "    def __init__(self, data, directory, transform = None, do_random_crop = True, itype = '.png'):\n",
    "        self.data      = data\n",
    "        self.directory = directory\n",
    "        self.transform = transform\n",
    "        self.do_random_crop = do_random_crop\n",
    "        self.itype = itype\n",
    "    # length\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    # get items    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.directory, self.data.loc[idx, 'id_code'] + self.itype)\n",
    "        image = cv2.imread(img_name)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = crop_black(image, tol = 7)\n",
    "        image = cv2.resize(image, (int(image_size), int(image_size)))\n",
    "        image = circle_crop(image, sigmaX = 10)\n",
    "        image = torch.tensor(image)\n",
    "        image = image.permute(2, 1, 0)\n",
    "        image    = self.transform(image)\n",
    "        label    = torch.tensor(self.data.loc[idx, 'diagnosis'])\n",
    "        return {'image': image, 'label': label}\n",
    "seed = 23\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f98c9954-7962-40c5-92cb-68e35538ea50",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35126, 2) (35108, 4)\n",
      "---------------\n",
      "0    25810\n",
      "2     5292\n",
      "1     2443\n",
      "3      873\n",
      "4      708\n",
      "Name: diagnosis, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('eye-cancer/trainLabels.csv')\n",
    "train.columns = ['id_code', 'diagnosis']\n",
    "\n",
    "# check shape\n",
    "print(train.shape, test.shape)\n",
    "print('-' * 15)\n",
    "print(train['diagnosis'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8fce6d96-7209-41e0-97f5-2bc0fb0e15d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4660cc994014b31ac7fd0e8d0293cf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_trans = transforms.Compose([transforms.ToPILImage(),\n",
    "                                   transforms.ToTensor(),\n",
    "                                  ])\n",
    "sample = Data(data       = train.iloc[0:10], \n",
    "                      directory  = 'eye-cancer/resized_train/resized_train',\n",
    "                      transform  = sample_trans,\n",
    "                      itype ='.jpeg')\n",
    "\n",
    "# data loader\n",
    "sample_loader = torch.utils.data.DataLoader(dataset     = sample, \n",
    "                                            batch_size  = 10, \n",
    "                                            shuffle     = False, \n",
    "                                            num_workers = 4)\n",
    "\n",
    "\n",
    "# placeholder\n",
    "image_stats = []\n",
    "\n",
    "# import loop\n",
    "for index, observation in tqdm(train.iterrows(), total = len(train)):\n",
    "    \n",
    "    # import image\n",
    "    img = cv2.imread('eye-cancer/resized_train/resized_train/{}.jpeg'.format(observation['id_code']))\n",
    "\n",
    "    # compute stats\n",
    "    height, width, channels = img.shape\n",
    "    ratio = width / height\n",
    "    \n",
    "    # save\n",
    "    image_stats.append(np.array((observation['diagnosis'], height, width, channels, ratio)))\n",
    "\n",
    "# construct DF\n",
    "image_stats = pd.DataFrame(image_stats)\n",
    "image_stats.columns = ['diagnosis', 'height', 'width', 'channels', 'ratio']\n",
    "\n",
    "batch_size = 16\n",
    "image_size = 256\n",
    "\n",
    "# train transformations\n",
    "train_trans = transforms.Compose([transforms.ToPILImage(),\n",
    "                                  transforms.RandomRotation((-360, 360)),\n",
    "                                  transforms.RandomHorizontalFlip(),\n",
    "                                  transforms.RandomVerticalFlip(),\n",
    "                                  transforms.ToTensor()\n",
    "                                 ])\n",
    "\n",
    "# validation transformations\n",
    "valid_trans = transforms.Compose([transforms.ToPILImage(),\n",
    "                                  transforms.ToTensor(),\n",
    "                                 ])\n",
    "\n",
    "# test transformations\n",
    "test_trans = valid_trans\n",
    "\n",
    "sample = EyeData(data       = train.iloc[0:10], \n",
    "                      directory  = '../input/diabetic-retinopathy-resized/resized_train/resized_train',\n",
    "                      transform  = train_trans,\n",
    "                      itype ='.jpeg')\n",
    "\n",
    "# data loader\n",
    "sample_loader = torch.utils.data.DataLoader(dataset     = sample, \n",
    "                                            batch_size  = batch_size, \n",
    "                                            shuffle     = True, \n",
    "                                            num_workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ee4baf0c-bb2b-4514-8691-dff6072a0998",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_classes = 5\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 5\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "data_train = train\n",
    "data_valid = train\n",
    "\n",
    "# create datasets\n",
    "train_dataset = EyeData(data      = data_train, \n",
    "                             directory = 'eye-cancer/resized_train/resized_train',\n",
    "                             transform = train_trans,\n",
    "                             itype ='.jpeg')\n",
    "\n",
    "# create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                           batch_size  = batch_size, \n",
    "                                           shuffle     = True, \n",
    "                                           num_workers = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83668f95-8136-4608-9cc2-49ad4740bf4f",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7d02f035-83cc-4c87-ada7-01b20be19664",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models import inception_v3\n",
    "\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "model_ft = inception_v3(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "612476cd-fecc-4fae-b1a6-592fe1683e11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "loss_criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "124db32b-9589-47e3-b379-953fd5c51e1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Calculated padded input size per channel: (4 x 4). Kernel size: (5 x 5). Kernel size can't be greater than actual input size",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# forward and backward pass\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 32\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     loss  \u001b[38;5;241m=\u001b[39m criterion(preds, labels)\n\u001b[1;32m     34\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/models/inception.py:166\u001b[0m, in \u001b[0;36mInception3.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m InceptionOutputs:\n\u001b[1;32m    165\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_input(x)\n\u001b[0;32m--> 166\u001b[0m     x, aux \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m     aux_defined \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maux_logits\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/models/inception.py:138\u001b[0m, in \u001b[0;36mInception3._forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mAuxLogits \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m--> 138\u001b[0m         aux \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAuxLogits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# N x 768 x 17 x 17\u001b[39;00m\n\u001b[1;32m    140\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMixed_7a(x)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/models/inception.py:386\u001b[0m, in \u001b[0;36mInceptionAux.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    384\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv0(x)\n\u001b[1;32m    385\u001b[0m \u001b[38;5;66;03m# N x 128 x 5 x 5\u001b[39;00m\n\u001b[0;32m--> 386\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;66;03m# N x 768 x 1 x 1\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# Adaptive average pooling\u001b[39;00m\n\u001b[1;32m    389\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39madaptive_avg_pool2d(x, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/models/inception.py:405\u001b[0m, in \u001b[0;36mBasicConv2d.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 405\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    406\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn(x)\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mrelu(x, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Calculated padded input size per channel: (4 x 4). Kernel size: (5 x 5). Kernel size can't be greater than actual input size"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    ### PREPARATION\n",
    "\n",
    "    # timer\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    # reset losses\n",
    "    trn_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "\n",
    "    # placeholders\n",
    "    fold_preds = np.zeros((len(data_valid), 5))\n",
    "\n",
    "\n",
    "    #TRAINING\n",
    "\n",
    "    # switch regime\n",
    "    model.train()\n",
    "\n",
    "    # loop through batches\n",
    "    for batch_i, data in enumerate(train_loader):\n",
    "\n",
    "        # extract inputs and labels\n",
    "        inputs = data['image']\n",
    "        labels = data['label'].view(-1)\n",
    "        inputs = inputs.to(device, dtype = torch.float)\n",
    "        labels = labels.to(device, dtype = torch.long)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward and backward pass\n",
    "        with torch.set_grad_enabled(True):\n",
    "            preds = model(inputs)\n",
    "            loss  = criterion(preds, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # compute loss\n",
    "        trn_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        \n",
    "    #INFERENCE\n",
    "\n",
    "    # switch regime\n",
    "    model.eval()\n",
    "    \n",
    "    # loop through batches\n",
    "    for batch_i, data in enumerate(train_loader):\n",
    "        \n",
    "        # extract inputs and labels\n",
    "        inputs = data['image']\n",
    "        labels = data['label'].view(-1)\n",
    "        inputs = inputs.to(device, dtype = torch.float)\n",
    "        labels = labels.to(device, dtype = torch.long)\n",
    "\n",
    "        # compute predictions\n",
    "        with torch.set_grad_enabled(False):\n",
    "            preds = model(inputs).detach()\n",
    "            fold_preds[batch_i * batch_size:(batch_i + 1) * batch_size, :] = preds.cpu().numpy()\n",
    "\n",
    "        # compute loss\n",
    "        loss      = criterion(preds, labels)\n",
    "        val_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "    # save predictions\n",
    "    oof_preds = fold_preds\n",
    "\n",
    "    # scheduler step\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "    #EVALUATION\n",
    "\n",
    "    # evaluate performance\n",
    "    fold_preds_round = fold_preds.argmax(axis = 1)\n",
    "    val_kappa = metrics.cohen_kappa_score(data_valid['diagnosis'], fold_preds_round.astype('int'), weights = 'quadratic')\n",
    "\n",
    "    # save perfoirmance values\n",
    "    val_kappas.append(val_kappa)\n",
    "    val_losses.append(val_loss / len(data_valid))\n",
    "    trn_losses.append(trn_loss / len(data_train))\n",
    "\n",
    "\n",
    "    #EARLY STOPPING\n",
    "\n",
    "    # display info\n",
    "    print('- epoch {}/{} | lr = {} | trn_loss = {:.4f} | val_loss = {:.4f} | val_kappa = {:.4f} | {:.2f} min'.format(\n",
    "        epoch + 1, max_epochs, scheduler.get_lr()[len(scheduler.get_lr()) - 1],\n",
    "        trn_loss / len(data_train), val_loss / len(data_valid), val_kappa,\n",
    "        (time.time() - epoch_start) / 60))\n",
    "\n",
    "    # check if there is any improvement\n",
    "    if epoch > 0:       \n",
    "        if val_kappas[epoch] < val_kappas[epoch - bad_epochs - 1]:\n",
    "            bad_epochs += 1\n",
    "        else:\n",
    "            bad_epochs = 0\n",
    "\n",
    "    # save model weights if improvement\n",
    "    if bad_epochs == 0:\n",
    "        oof_preds_best = oof_preds.copy()\n",
    "        torch.save(model.state_dict(), 'models/model_{}.bin'.format(model_name))\n",
    "\n",
    "    # break if early stop\n",
    "    if bad_epochs == early_stop:\n",
    "        print('Early stopping. Best results: loss = {:.4f}, kappa = {:.4f} (epoch {})'.format(\n",
    "            np.min(val_losses), val_kappas[np.argmin(val_losses)], np.argmin(val_losses) + 1))\n",
    "        print('')\n",
    "        break\n",
    "\n",
    "    # break if max epochs\n",
    "    if epoch == (max_epochs - 1):\n",
    "        print('Did not met early stopping. Best results: loss = {:.4f}, kappa = {:.4f} (epoch {})'.format(\n",
    "            np.min(val_losses), val_kappas[np.argmin(val_losses)], np.argmin(val_losses) + 1))\n",
    "        print('')\n",
    "        break\n",
    "\n",
    "\n",
    "# load best predictions\n",
    "oof_preds = oof_preds_best\n",
    "\n",
    "# print performance\n",
    "print('')\n",
    "print('Finished in {:.2f} minutes'.format((time.time() - cv_start) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1979db96-fc0f-4e3e-9017-c1ba11088b4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_accuracy(model, loader):\n",
    "    model.eval()\n",
    "    \n",
    "    correct_output = 0\n",
    "    total_output = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in tqdm(loader):\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "            \n",
    "            score = model(x)\n",
    "            _,predictions = score.max(1)\n",
    "            \n",
    "            correct_output += (y==predictions).sum()\n",
    "            total_output += predictions.shape[0]\n",
    "    model.train()\n",
    "    print(f\"out of {total_output} , total correct: {correct_output} with an accuracy of {float(correct_output/total_output)*100}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "46304ddd-c0b2-4bd9-9ac8-8fe17f2e7785",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9db0e83c91a545239819b98e9f92734b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out of 5000 , total correct: 3677 with an accuracy of 73.53999614715576\n"
     ]
    }
   ],
   "source": [
    "check_accuracy(model, test_dataloader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82304884-02a6-4356-97bc-418127e18ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "73.17999601364136"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fe2cbf-58b7-4029-ab7d-3ee3d9954de4",
   "metadata": {},
   "source": [
    "## Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0c1bf0c0-c63d-4f80-bcb8-58d8d79a32f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: captum in /opt/conda/lib/python3.10/site-packages (0.6.0)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from captum) (3.5.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from captum) (1.24.4)\n",
      "Requirement already satisfied: torch>=1.6 in /opt/conda/lib/python3.10/site-packages (from captum) (2.0.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (4.3.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (1.10.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->captum) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6->captum) (68.0.0)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6->captum) (0.41.0)\n",
      "Requirement already satisfied: cmake in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.6->captum) (3.27.2)\n",
      "Requirement already satisfied: lit in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.6->captum) (16.0.6)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum) (1.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum) (10.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->captum) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->captum) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6->captum) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6->captum) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: cleverhans in /opt/conda/lib/python3.10/site-packages (4.0.0)\n",
      "Requirement already satisfied: nose in /opt/conda/lib/python3.10/site-packages (from cleverhans) (1.3.7)\n",
      "Requirement already satisfied: pycodestyle in /opt/conda/lib/python3.10/site-packages (from cleverhans) (2.8.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from cleverhans) (1.9.1)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from cleverhans) (3.5.2)\n",
      "Requirement already satisfied: mnist in /opt/conda/lib/python3.10/site-packages (from cleverhans) (0.2.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from cleverhans) (1.24.4)\n",
      "Requirement already satisfied: tensorflow-probability in /opt/conda/lib/python3.10/site-packages (from cleverhans) (0.21.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from cleverhans) (1.3.1)\n",
      "Requirement already satisfied: easydict in /opt/conda/lib/python3.10/site-packages (from cleverhans) (1.10)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from cleverhans) (1.4.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from cleverhans) (1.16.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->cleverhans) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->cleverhans) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->cleverhans) (1.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->cleverhans) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->cleverhans) (10.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->cleverhans) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->cleverhans) (2.8.2)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from tensorflow-probability->cleverhans) (5.1.1)\n",
      "Requirement already satisfied: cloudpickle>=1.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow-probability->cleverhans) (2.2.1)\n",
      "Requirement already satisfied: gast>=0.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow-probability->cleverhans) (0.5.4)\n",
      "Requirement already satisfied: dm-tree in /opt/conda/lib/python3.10/site-packages (from tensorflow-probability->cleverhans) (0.1.8)\n",
      "Requirement already satisfied: typing-extensions<4.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-probability->cleverhans) (4.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'clear_output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcleverhans\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcarlini_wagner_l2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m carlini_wagner_l2\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcleverhans\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprojected_gradient_descent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m     projected_gradient_descent\n\u001b[1;32m     17\u001b[0m )\n\u001b[0;32m---> 18\u001b[0m \u001b[43mclear_output\u001b[49m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clear_output' is not defined"
     ]
    }
   ],
   "source": [
    "!pip install captum\n",
    "!pip install cleverhans\n",
    "from captum.attr import IntegratedGradients\n",
    "from captum.attr import Saliency\n",
    "from captum.attr import DeepLift\n",
    "from captum.attr import NoiseTunnel\n",
    "from captum.attr import GradientShap\n",
    "from captum.attr import GuidedGradCam\n",
    "from captum.attr import LimeBase\n",
    "from captum.attr import KernelShap\n",
    "from captum.attr import Occlusion\n",
    "from captum.attr import visualization as viz\n",
    "from cleverhans.torch.attacks.fast_gradient_method import fast_gradient_method\n",
    "from cleverhans.torch.attacks.carlini_wagner_l2 import carlini_wagner_l2\n",
    "from cleverhans.torch.attacks.projected_gradient_descent import (\n",
    "    projected_gradient_descent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2d686945-d6fb-4964-bb2a-c3730e1036c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_mean_abs_dev(attr): \n",
    "    scores = []\n",
    "    for i in range(len(attr)):\n",
    "        a = attr[i].flatten()\n",
    "        avg = np.mean(a)\n",
    "        deviation = a - avg \n",
    "        absolute_deviation = np.abs(deviation)\n",
    "        result = np.mean(absolute_deviation)\n",
    "        scores.append(result)\n",
    "    return scores    \n",
    "def compute_median_abs_dev(attr): \n",
    "    scores = []\n",
    "    for i in range(len(attr)):\n",
    "        a = attr[i].flatten()\n",
    "        med = np.median(a)\n",
    "        deviation = a - med \n",
    "        abs_deviation = np.abs(deviation)\n",
    "        result = np.median(abs_deviation)\n",
    "        scores.append(result)\n",
    "    return scores \n",
    "def compute_iqr(attr):\n",
    "    #inter-quartile range\n",
    "    scores = []\n",
    "    for i in range(len(attr)):\n",
    "        a = attr[i].flatten()\n",
    "        score_75 = np.percentile(a, 75)\n",
    "        score_25 = np.percentile(a, 25)\n",
    "        score_qt = score_75 - score_25\n",
    "        scores.append(score_qt)\n",
    "    return scores\n",
    "    \n",
    "def compute_coef_var(attr):\n",
    "    scores = []\n",
    "    for i in range(len(attr)):\n",
    "        a = attr[i].flatten()\n",
    "        m = np.mean(a)\n",
    "        st = np.std(attr[i])\n",
    "        sc = m/st\n",
    "        scores.append(sc)\n",
    "    return scores\n",
    "\n",
    "def compute_coef_iqr(attr):\n",
    "    scores = []\n",
    "    for i in range(len(attr)):\n",
    "        a = attr[i].flatten()\n",
    "        score_75 = np.percentile(a, 75)\n",
    "        score_25 = np.percentile(a, 25)\n",
    "        score_qt = (score_75 - score_25)/(score_75 + score_25)\n",
    "        scores.append(score_qt)\n",
    "    return scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d249bd20-2316-430f-8e95-5211f99a6be9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected more than 1 value per channel when training, got input size torch.Size([1, 768, 1, 1])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m igcoef_iqr_ben \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m compute_coef_iqr(a_batch_benign)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# torch.cuda.empty_cache()\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m images_pgd \u001b[38;5;241m=\u001b[39m \u001b[43mfast_gradient_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m _, y_pred_pgd \u001b[38;5;241m=\u001b[39m model(input1)\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# index = (y_pred_pgd != labels)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# pgd_images = images_pgd[index]\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# y_pred_pgd = y_pred_pgd[index]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/cleverhans/torch/attacks/fast_gradient_method.py:77\u001b[0m, in \u001b[0;36mfast_gradient_method\u001b[0;34m(model_fn, x, eps, norm, clip_min, clip_max, y, targeted, sanity_checks)\u001b[0m\n\u001b[1;32m     74\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat)\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;66;03m# Using model predictions as ground truth to avoid label leaking\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m     _, y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(\u001b[43mmodel_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m     80\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/models/inception.py:166\u001b[0m, in \u001b[0;36mInception3.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m InceptionOutputs:\n\u001b[1;32m    165\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_input(x)\n\u001b[0;32m--> 166\u001b[0m     x, aux \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m     aux_defined \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maux_logits\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/models/inception.py:138\u001b[0m, in \u001b[0;36mInception3._forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mAuxLogits \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m--> 138\u001b[0m         aux \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAuxLogits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# N x 768 x 17 x 17\u001b[39;00m\n\u001b[1;32m    140\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMixed_7a(x)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/models/inception.py:386\u001b[0m, in \u001b[0;36mInceptionAux.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    384\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv0(x)\n\u001b[1;32m    385\u001b[0m \u001b[38;5;66;03m# N x 128 x 5 x 5\u001b[39;00m\n\u001b[0;32m--> 386\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;66;03m# N x 768 x 1 x 1\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# Adaptive average pooling\u001b[39;00m\n\u001b[1;32m    389\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39madaptive_avg_pool2d(x, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/models/inception.py:406\u001b[0m, in \u001b[0;36mBasicConv2d.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    405\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x)\n\u001b[0;32m--> 406\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mrelu(x, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:2448\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2435\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2436\u001b[0m         batch_norm,\n\u001b[1;32m   2437\u001b[0m         (\u001b[38;5;28minput\u001b[39m, running_mean, running_var, weight, bias),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2445\u001b[0m         eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m   2446\u001b[0m     )\n\u001b[1;32m   2447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[0;32m-> 2448\u001b[0m     \u001b[43m_verify_batch_size\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2450\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbatch_norm(\n\u001b[1;32m   2451\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39menabled\n\u001b[1;32m   2452\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:2416\u001b[0m, in \u001b[0;36m_verify_batch_size\u001b[0;34m(size)\u001b[0m\n\u001b[1;32m   2414\u001b[0m     size_prods \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m size[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m   2415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_prods \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 2416\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected more than 1 value per channel when training, got input size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(size))\n",
      "\u001b[0;31mValueError\u001b[0m: Expected more than 1 value per channel when training, got input size torch.Size([1, 768, 1, 1])"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "dataiter = iter(train_dataloader)\n",
    "\n",
    "igmedianAbs_ben = []\n",
    "igmeanAbs_ben = []\n",
    "igiqr_ben = []\n",
    "igcoef_var_ben=[]\n",
    "igcoef_iqr_ben = []\n",
    "\n",
    "igmedianAbs_bena = []\n",
    "igmeanAbs_bena = []\n",
    "igiqr_bena = []\n",
    "igcoef_var_bena=[]\n",
    "igcoef_iqr_bena = []\n",
    "\n",
    "for i in range(5):\n",
    "    images, labels = next(dataiter)\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    for ind in range(32):\n",
    "        input1 = images[ind].unsqueeze(0)\n",
    "        x_logits = model(images)\n",
    "        ig = IntegratedGradients(model)\n",
    "        a_batch_benign = ig.attribute(input1, target=labels[ind]).sum(axis=1).cpu().detach().numpy()\n",
    "        igmeanAbs_ben += compute_mean_abs_dev(a_batch_benign)\n",
    "        igmedianAbs_ben += compute_median_abs_dev(a_batch_benign)\n",
    "        igiqr_ben += compute_iqr(a_batch_benign)\n",
    "        igcoef_var_ben += compute_coef_var(a_batch_benign)\n",
    "        igcoef_iqr_ben += compute_coef_iqr(a_batch_benign)\n",
    "        # torch.cuda.empty_cache()\n",
    "        images_pgd = fast_gradient_method(model, input1, 0.1, np.inf)\n",
    "        _, y_pred_pgd = model(input1).max(1)\n",
    "        # index = (y_pred_pgd != labels)\n",
    "        # pgd_images = images_pgd[index]\n",
    "        # y_pred_pgd = y_pred_pgd[index]\n",
    "        a_batch_attack = ig.attribute(inputs=images_pgd, target=y_pred_pgd).sum(axis=1).cpu().detach().numpy()\n",
    "        igmeanAbs_bena += compute_mean_abs_dev(a_batch_attack)\n",
    "        igmedianAbs_bena += compute_median_abs_dev(a_batch_attack)\n",
    "        igiqr_bena += compute_iqr(a_batch_attack)\n",
    "        igcoef_var_bena += compute_coef_var(a_batch_attack)\n",
    "        igcoef_iqr_bena += compute_coef_iqr(a_batch_attack)\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4d02721a-eab1-417d-bebd-e5b4e154c238",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(igiqr_ben, label='clean', color='blue')\n",
    "plt.hist(igiqr_bena, label='adv', alpha=0.7, color='red')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.g4dn.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
